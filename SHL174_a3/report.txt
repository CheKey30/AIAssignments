Name: Shuchen Liu
Pitt id: SHL174@pitt.edu
Tool: Scikit-learn, numpy, pandas

Output:
================Task 1================
Model 1:
Mean squared error	25.043588205218548
--------------------
Model 2:
Mean squared error	22.73358077422145
================Task 2================
Model 1:
Accuracy	0.328125	Macro_F1	0.19400456621004566	Macro_Precision	0.45925925925925926	Macro_Recall	0.23246753246753243
Category	teacher	F1	0.23999999999999996	Precision	0.5	Recall	0.15789473684210525
Category	health	F1	0	Precision	0.0	Recall	0.0
Category	service	F1	0.4383561643835616	Precision	0.2962962962962963	Recall	0.8421052631578947
Category	at_home	F1	0.125	Precision	0.5	Recall	0.07142857142857142
Category	other	F1	0.16666666666666669	Precision	1.0	Recall	0.09090909090909091
--------------------
Model 2:
Accuracy	0.296875	Macro_F1	0.14947368421052634	Macro_Precision	0.45964912280701753	Macro_Recall	0.2114149008885851
Category	teacher	F1	0	Precision	0.0	Recall	0.0
Category	health	F1	0	Precision	0.0	Recall	0.0
Category	service	F1	0.44736842105263164	Precision	0.2982456140350877	Recall	0.8947368421052632
Category	at_home	F1	0.13333333333333333	Precision	1.0	Recall	0.07142857142857142
Category	other	F1	0.16666666666666669	Precision	1.0	Recall	0.09090909090909091
================Task 3================
Model 1:
Accuracy	0.28125	Hamming loss	0.3046875
--------------------
Model 2:
Accuracy	0.328125	Hamming loss	0.3125

Report:

Task 1:

- What features do you choose to use and why chose them?
  a. Feature 1: I used ridge to do the feature selection, some of them would influce more and others would influence less.
  b. Feature 2: 
  c. Feature 3: 
  ...

- How do you use these features? For example, original value, normalized value, log value, one hot vector, etc.
  a. Feature 1: I used one-hot encoding to change all category features in this way those String labels can be used as number but not be influenced by the big or small of the number.
  b. Feature 2: I normalized the value of those continuous variables.
  c. Feature 3: 
  ...

- Model 1
  a. Model name: Ridge
  b. Parameters that I tried and the corresponding performance on training data with 10 fold cross-validation. the alpha of the ridge
  c. Final performance on testing data. Mean squared error	25.043588205218548
  d. Running time of training the model. several seconds

- Model 2
  a. Model name: random forest regression
  b. Parameters that I tried and the corresponding performance on training data with 10 fold cross-validation. the n_estimators
  c. Final performance on testing data. Mean squared error	22.73358077422145
  d. Running time of training the model. several seconds



Task 2:

- What features do you choose to use and why chose them?
  a. Feature 1: I used all features to do the task.
  b. Feature 2: 
  c. Feature 3: 
  ...

- How do you use these features? For example, original value, normalized value, log value, one hot vector, etc.
  a. Feature 1:  I used one-hot encoding to change all category features in this way those String labels can be used as number but not be influenced by the big or little of the number.
  b. Feature 2: 
  c. Feature 3: 
  ...

- Model 1
  a. Model name: KNN
  b. Parameters that I tried and the corresponding performance on training data with 10 fold cross-validation. the k value of KNN
  c. Final performance on testing data. Accuracy	0.328125
  d. Running time of training the model. several seconds

- Model 2
  a. Model name: random forest classifier
  b. Parameters that I tried and the corresponding performance on training data with 10 fold cross-validation. the n_estimators
  c. Final performance on testing data. Accuracy	0.296875
  d. Running time of training the model. several seconds




Task 3:

- What features do you choose to use and why chose them?
  a. Feature 1: I used all features.
  b. Feature 2: 
  c. Feature 3: 
  ...

- How do you use these features? For example, original value, normalized value, log value, one hot vector, etc.
  a. Feature 1: I used one-hot encoding to change all category features in this way those String labels can be used as number but not be influenced by the big or little of the number.
  b. Feature 2: I splited feature edusupport into 4 features as the label.
  c. Feature 3: 
  ...

- Model 1
  a. Model name: SVM with OneVsRestClassifier
  b. Parameters that I tried and the corresponding performance on training data with 10 fold cross-validation. the error range c of SVM
  c. Final performance on testing data. Accuracy	0.28125
  d. Running time of training the model. several seconds 

- Model 2
  a. Model name: random forest classifier with OneVsRestClassifier
  b. Parameters that I tried and the corresponding performance on training data with 10 fold cross-validation. the n_estimators
  c. Final performance on testing data. Accuracy	0.328125
  d. Running time of training the model. several seconds